{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" BertGPT.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1LcZn8v3fk4DTpHl1Kt_0LJiwc97MaWbX","authorship_tag":"ABX9TyPF7MUYzWCT1u2jJiaJ6LgU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oQ9SD-cUNV7s"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv( './drive/MyDrive/Data/dontpatronizeme_pcl.tsv', sep = '\\t', names=['id','info','country', 'text','label'] )\n","df"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtJZCW_a8Ltf","executionInfo":{"status":"ok","timestamp":1643260250857,"user_tz":-330,"elapsed":23041,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}},"outputId":"47e67460-64ea-49f5-9641-86a7c33b229b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install sentencepiece transformers\n"],"metadata":{"id":"klT3827qSfFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test_= pd.read_csv( './drive/MyDrive/Data/pcl_test.tsv', sep = '\\t', names=['seq','id','info','country', 'text','label_'] )\n","df_test_"],"metadata":{"id":"pfK93Kr0r0o7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test= df_test_[['text','label_']]\n","df_test"],"metadata":{"id":"Fjxr21hCs1Cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test['label_'] = 1\n","df_test"],"metadata":{"id":"9k5_VR-F3fbZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","from transformers import GPT2Tokenizer, GPT2Model\n","import pandas as pd\n","df = pd.read_csv('./drive/MyDrive/Data/dontpatronizeme_pcl.tsv', sep = '\\t', names=['id','info','country', 'text','label'] )\n","\n","df = df.dropna(inplace = False)\n","\n","df = df.reset_index(drop = True)\n","df.info()\n","\n","# Importing the libraries needed\n","import torch\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","df_final = df[['text','label']]\n","#df_data = data.dropna(subset=['id','info','country'])\n","df_final                  \n","\n","df_final.shape\n","#df_final.columns\n","df_final['label_']= [ 0 if (y == 1 or y == 0) else 1 for y in df_final['label']]\n","df_final\n","df_final.drop('label', axis = 1, inplace= True)\n","print(df_final)\n","\n","nclasses = len(list(df_final.label_.unique()))\n","nclasses\n","\n","my_classes = {c:i for i, c in enumerate(list(df_final.label_.unique()))}\n","df_final['label_'] = [my_classes[l] for l in df_final.label_]\n","\n","MAX_LEN= 512\n","TRAIN_BS = 8\n","VALID_BS = 8\n","EPOCHS = 1\n","LR =  1e-05\n","#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","class PclData(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        print(self.len); print(self.data)\n","        \n","    def __getitem__(self, index):# function compulsory\n","        sent = self.data.text[index]\n","       # sent = \" \".join(sent.split())\n","        inputs = self.tokenizer.encode_plus(\n","        sent, \n","        None,\n","        add_special_tokens = True,\n","        max_length=self.max_len,\n","        padding = 'max_length',\n","        return_token_type_ids = True,\n","        truncation=True\n","        )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        \n","        return { 'ids': torch.tensor(ids, dtype=torch.long),\n","               'mask': torch.tensor(mask, dtype=torch.long),\n","                'targets': torch.tensor(self.data.label_[index], dtype=torch.long)\n","                \n","               }\n","    \n","    def __len__(self):\n","        return self.len\n","        "],"metadata":{"id":"d3VtkE7WTMP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_size = 0.8\n","train_dataset = df_final.sample(frac= train_size, random_state=20)\n","val_dataset = df_final.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","test_dataset = df_test\n","\n","#train_dataset_1 = [ 0 if (y == 1 or y == 0) else 1 for y in train_dataset ]\n","#test_dataset_1 = [ 0 if (y == 1 or y == 0) else 1 for y in test_dataset ]\n","\n","\n","print(\"FULL Dataset: {}\".format(df_final.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","training_set = PclData(train_dataset, tokenizer, MAX_LEN)\n","val_set = PclData(val_dataset, tokenizer, MAX_LEN)\n","test_set= PclData(test_dataset, tokenizer, MAX_LEN)\n","\n","train_params = {'batch_size': TRAIN_BS,\n","               'shuffle':True,\n","               'num_workers': 2}\n","val_params = {'batch_size': VALID_BS,\n","              'shuffle': False,\n","              'num_workers': 2}\n","\n","trainloader = DataLoader(training_set, **train_params)\n","valloader = DataLoader(val_set, **val_params)\n","testloader = DataLoader(test_set, **val_params)"],"metadata":{"id":"zOf9q9aFTY2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GptModelClass (torch.nn.Module):\n","    def __init__(self, nclasses):\n","        super(GptModelClass, self).__init__()\n","        self.l1 = GPT2Tokenizer.from_pretrained('gpt2')\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.3)\n","        self.classifier = torch.nn.Linear(768, nclasses)\n","\n","    def forward(self, input_ids, attention_mask):\n","        with torch.autograd.no_grad():\n","            output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = output_1[0]\n","        pooler = hidden_state[:, 0]\n","        pooler = self.pre_classifier(pooler)\n","        pooler = torch.nn.ReLU()(pooler)\n","        pooler = self.dropout(pooler)\n","        output = self.classifier(pooler)\n","        return output"],"metadata":{"id":"Ry83xgwMn_jW","executionInfo":{"status":"ok","timestamp":1643260573052,"user_tz":-330,"elapsed":564,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model = GptModelClass(nclasses)\n","model.to(device)"],"metadata":{"id":"nxXvGk81pBey"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","wt_array =len(df_final['text'])/(len(set(df_final['label_']))*(np.bincount(df_final['label_'])))\n","wt_array"],"metadata":{"id":"ONUmTWF_7Yp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_weights=torch.FloatTensor(wt_array)"],"metadata":{"id":"nw83X3vw7pIC","executionInfo":{"status":"ok","timestamp":1643260608946,"user_tz":-330,"elapsed":618,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["loss_function = torch.nn.CrossEntropyLoss(weight=class_weights)\n","optimizer = torch.optim.Adam(params =model.parameters(), lr=LR)\n","def calculate_acc(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct\n"],"metadata":{"id":"umH-JoN3pVD7","executionInfo":{"status":"ok","timestamp":1643260615500,"user_tz":-330,"elapsed":776,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter('runs/textclassify_experiment_1')\n","\n"],"metadata":{"id":"ZX14KNQmrDhQ","executionInfo":{"status":"ok","timestamp":1643260628935,"user_tz":-330,"elapsed":6467,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def train(epoch):\n","    tr_loss = 0\n","    n_correct = 0\n","    nb_tr_steps = 0\n","    nb_tr_examples = 0\n","    \n","    model.train()\n","    for _, data in tqdm(enumerate(trainloader, 0)):\n","        ids = data['ids'].to(device, dtype=torch.long)\n","        mask = data['mask'].to(device, dtype=torch.long)\n","        targets = data['targets'].to(device, dtype=torch.long)\n","        \n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, targets)\n","        tr_loss += loss.item()\n","        big_val, big_idx = torch.max(outputs.data, dim=1)\n","        n_correct += calculate_acc(big_idx, targets)\n","        \n","        nb_tr_steps += 1\n","        nb_tr_examples += targets.size(0)\n","        \n","        if _ % 50 == 0:\n","            \n","            loss_step = tr_loss/nb_tr_steps\n","            acc_step = (n_correct*100)/nb_tr_examples\n","            print(f'Training Loss per 50 steps: {loss_step}, Training Accuracy: {acc_step}')\n","            writer.add_scalar('training_loss', loss_step, epoch*len(trainloader) +_)\n","            \n","            \n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","   # print(f'Total Accuracy Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_acc = (n_correct*100)/nb_tr_examples\n","    print(f'Training Loss Epoch: {epoch_loss}, Training Accuracy Epoch: {epoch_acc}')\n","    valid(model, valloader)\n","    return"],"metadata":{"id":"6PfZBRYnrHKI","executionInfo":{"status":"ok","timestamp":1643260633318,"user_tz":-330,"elapsed":618,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Validation\n","\n","def valid(model, testloader):\n","    model.eval()\n","    n_correct =0; n_wrong =0; total=0; tr_loss =0; nb_tr_steps =0; nb_tr_examples=0\n","    y_pred, y_true = [],[]\n","    with torch.no_grad():\n","        for _, data in enumerate(testloader, 0):\n","            ids = data['ids'].to(device, dtype=torch.long)\n","            mask = data['mask'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype=torch.long)\n","            \n","            outputs = model(ids, mask)\n","            loss = loss_function(outputs, targets)\n","            tr_loss += loss.item()\n","            big_val, big_idx = torch.max(outputs.data, dim=1)\n","            n_correct += calculate_acc(big_idx, targets)\n","\n","            y_true.extend(targets.cpu().detach().numpy())\n","      \n","            y_pred.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy())\n","            nb_tr_steps += 1\n","            nb_tr_examples += targets.size(0)\n","            \n","            if _ % 100 == 0:\n","                loss_step = tr_loss/nb_tr_steps\n","                acc_step = (n_correct*100)/nb_tr_examples\n","                #print(f'Validation Loss per 100 steps: {loss_step}, Validation Accuracy per: {acc_step}')   \n","        \n","    epoch_loss = tr_loss/nb_tr_steps\n","    epoch_acc = (n_correct*100)/nb_tr_examples\n","    print(f'Validation Loss Epoch: {epoch_loss}, Validation Accuracy Epoch: {epoch_acc}')\n","    \n","    return epoch_acc, y_true, y_pred"],"metadata":{"id":"ONoUGedzrOCa","executionInfo":{"status":"ok","timestamp":1643260641926,"user_tz":-330,"elapsed":912,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["for epoch in range(EPOCHS):\n","    train(epoch)"],"metadata":{"id":"5280kYfwrThg","colab":{"base_uri":"https://localhost:8080/","height":692},"outputId":"77de1882-38dc-4851-e632-63a8c9a79ecd","executionInfo":{"status":"error","timestamp":1643260650332,"user_tz":-330,"elapsed":794,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]Using pad_token, but it is not set yet.\n","Using pad_token, but it is not set yet.\n","Using pad_token, but it is not set yet.\n","Using pad_token, but it is not set yet.\n","0it [00:00, ?it/s]Using pad_token, but it is not set yet.\n","\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c6928e977afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-17-4687daf06ebc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-8-c7576053f6bd>\", line 62, in __getitem__\n    truncation=True\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2508, in encode_plus\n    **kwargs,\n  File \"/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\", line 2314, in _get_padding_truncation_strategies\n    \"Asking to pad but the tokenizer does not have a padding token. \"\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"]}]},{"cell_type":"code","source":["acc, y_true, y_pred = valid(model, testloader)"],"metadata":{"id":"XRlfHpozrXDW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643094006337,"user_tz":-330,"elapsed":134747,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}},"outputId":"f681fea6-1bf7-4400-9fc4-2bbd505e694e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss per 100 steps: 0.6403908133506775, Validation Accuracy per: 50.0\n","Validation Loss per 100 steps: 0.4539571699529591, Validation Accuracy per: 72.52475247524752\n","Validation Loss per 100 steps: 0.4451032114859244, Validation Accuracy per: 74.75124378109453\n","Validation Loss Epoch: 0.4503463284318684, Validation Accuracy Epoch: 74.16427889207259\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","accuracy = confusion_matrix(y_true, y_pred)\n","print(accuracy)\n","from sklearn.metrics import classification_report\n","print(classification_report(y_true, y_pred))"],"metadata":{"id":"nuypq3rGub0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643094026908,"user_tz":-330,"elapsed":386,"user":{"displayName":"Abhishek kumar Singh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03453965687970289686"}},"outputId":"daad5f80-3d3d-4417-9e19-08cefbff3df3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1368  516]\n"," [  25  185]]\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.73      0.83      1884\n","           1       0.26      0.88      0.41       210\n","\n","    accuracy                           0.74      2094\n","   macro avg       0.62      0.80      0.62      2094\n","weighted avg       0.91      0.74      0.79      2094\n","\n"]}]},{"cell_type":"code","source":["f =open('bert_roberta_small_test_1.txt', 'w')\n","for i in y_pred:\n","  print(i, file = f )\n","f.close()\n"],"metadata":{"id":"dLeClJNDv3Bz"},"execution_count":null,"outputs":[]}]}