{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2990092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7997bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046bc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../Data/dontpatronizeme_pcl.tsv', sep = '\\t', names=['id','info','country', 'text','label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12638ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>info</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>White House press secretary Sean Spicer said t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\" Just like we received migrants fleeing El Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       info country  \\\n",
       "0  @@24942188   hopeless      ph   \n",
       "1  @@21968160    migrant      gh   \n",
       "2  @@16584954  immigrant      ie   \n",
       "3   @@7811231   disabled      nz   \n",
       "4   @@1494111    refugee      ca   \n",
       "\n",
       "                                                text  label  \n",
       "0  We 're living in times of absolute insanity , ...      0  \n",
       "1  In Libya today , there are countless number of...      0  \n",
       "2  White House press secretary Sean Spicer said t...      0  \n",
       "3  Council customers only signs would be displaye...      0  \n",
       "4  \" Just like we received migrants fleeing El Sa...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdfd4e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8529\n",
       "1     947\n",
       "3     458\n",
       "4     391\n",
       "2     144\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3370f5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10468 entries, 0 to 10467\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       10468 non-null  object\n",
      " 1   info     10468 non-null  object\n",
      " 2   country  10468 non-null  object\n",
      " 3   text     10468 non-null  object\n",
      " 4   label    10468 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 409.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(inplace = False)\n",
    "\n",
    "df = df.reset_index(drop = True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44142b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af630dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de073c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10468, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df[['text','label']]\n",
    "#df_data = data.dropna(subset=['id','info','country'])\n",
    "df_final                  \n",
    "\n",
    "df_final.shape\n",
    "#df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc452071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PREETA~1\\AppData\\Local\\Temp/ipykernel_3772/4051987690.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['label_']= [ 0 if (y == 1 or y == 0) else 1 for y in df_final['label']]\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White House press secretary Sean Spicer said t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" Just like we received migrants fleeing El Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_\n",
       "0  We 're living in times of absolute insanity , ...       0\n",
       "1  In Libya today , there are countless number of...       0\n",
       "2  White House press secretary Sean Spicer said t...       0\n",
       "3  Council customers only signs would be displaye...       0\n",
       "4  \" Just like we received migrants fleeing El Sa...       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['label_']= [ 0 if (y == 1 or y == 0) else 1 for y in df_final['label']]\n",
    "df_final\n",
    "df_final.drop('label', axis = 1, inplace= True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fed63278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nclasses = len(list(df_final.label_.unique()))\n",
    "nclasses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475c9d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PREETA~1\\AppData\\Local\\Temp/ipykernel_3772/40243744.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['label'] = [my_classes[l] for l in df_final.label_]\n"
     ]
    }
   ],
   "source": [
    "my_classes = {c:i for i, c in enumerate(list(df_final.label_.unique()))}\n",
    "df_final['label'] = [my_classes[l] for l in df_final.label_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c9cc945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9475\n",
       "1     993\n",
       "Name: label_, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.label_.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6373fe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9475\n",
       "1     993\n",
       "Name: label_, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de613e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN= 512\n",
    "TRAIN_BS = 8\n",
    "VALID_BS = 8\n",
    "EPOCHS = 1\n",
    "LR =  1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ccd76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "885d29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanyData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):# function compulsory\n",
    "        sent = str(self.df.text[index])\n",
    "        sent= \" \".join(review.split())\n",
    "        inputs = self.tokenizer.encode_plus(sent\n",
    "        , \n",
    "        None,\n",
    "        add_special_tokens = True,\n",
    "        max_length=self.max_len,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True,\n",
    "        truncation=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        return { 'ids': torch.tensor(ids, dtype=torch.long),\n",
    "               'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.df.label_[index], dtype=torch.long)\n",
    "                \n",
    "               }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09346137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (10468, 3)\n",
      "TRAIN Dataset: (8374, 3)\n",
      "TEST Dataset: (2094, 3)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset = df_final.sample(frac= train_size, random_state=20)\n",
    "test_dataset = df_final.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_final.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CompanyData(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CompanyData(test_dataset, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54c4d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BS,\n",
    "               'shuffle':True,\n",
    "               'num_workers': 2}\n",
    "test_params = {'batch_size': VALID_BS,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 2}\n",
    "\n",
    "trainloader = DataLoader(training_set, **train_params)\n",
    "testloader = DataLoader(testing_set, **test_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16596b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self, nclasses):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, nclasses)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d9e7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistillBERTClass(nclasses)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7971600",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef0ebe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acc(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c51f4031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26485d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/textclassify_experiment_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecacc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(trainloader, 0)):\n",
    "        ids = df['ids'].to(device, dtype=torch.long)\n",
    "        mask = df['mask'].to(device, dtype=torch.long)\n",
    "        targets = df['targets'].to(device, dtype=torch.long)\n",
    "        \n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_acc(big_idx, targets)\n",
    "        \n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "        \n",
    "        if _ % 50 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            acc_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f'Training Loss per 50 steps: {loss_step}, Training Accuracy: {acc_step}')\n",
    "            writer.add_scalar('training_loss', loss_step, epoch*len(trainloader) +_)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Total Accuracy Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
    "    print(f'Training Loss Epoch: {epoch_loss}, Training Accuracy Epoch: {epoch_acc}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36f43adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "def valid(model, testloader):\n",
    "    model.eval()\n",
    "    n_correct =0; n_wrong =0; total=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testloader, 0):\n",
    "            ids = df['ids'].to(device, dtype=torch.long)\n",
    "            mask = df['mask'].to(device, dtype=torch.long)\n",
    "            targets = df['targets'].to(device, dtype=torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calculate_acc(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "            \n",
    "            if _ % 100 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                acc_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f'Validation Loss per 100 steps: {loss_step}, Validation Accuracy per: {acc_step}')   \n",
    "        \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
    "    print(f'Validation Loss Epoch: {epoch_loss}, Validation Accuracy Epoch: {epoch_acc}')\n",
    "    \n",
    "    return epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc73489",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = valid(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6c03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
