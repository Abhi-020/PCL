{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ9SD-cUNV7s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv( './drive/MyDrive/Data/dontpatronizeme_pcl.tsv', sep = '\\t', names=['id','info','country', 'text','label'] )\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece transformers"
      ],
      "metadata": {
        "id": "klT3827qSfFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./drive/MyDrive/Data/dontpatronizeme_pcl.tsv', sep = '\\t', names=['id','info','country', 'text','label'] )\n",
        "df = df.dropna(inplace = False)\n",
        "\n",
        "df = df.reset_index(drop = True)\n",
        "df.info()\n",
        "\n",
        "# Importing the libraries needed\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "df_final = df[['text','label']]\n",
        "#df_data = data.dropna(subset=['id','info','country'])\n",
        "df_final                  \n",
        "\n",
        "df_final.shape\n",
        "#df_final.columns\n",
        "df_final['label_']= [ 0 if (y == 1 or y == 0) else 1 for y in df_final['label']]\n",
        "df_final\n",
        "df_final.drop('label', axis = 1, inplace= True)\n",
        "print(df_final)\n",
        "\n",
        "nclasses = len(list(df_final.label_.unique()))\n",
        "nclasses\n",
        "\n",
        "my_classes = {c:i for i, c in enumerate(list(df_final.label_.unique()))}\n",
        "df_final['label_'] = [my_classes[l] for l in df_final.label_]\n",
        "\n",
        "MAX_LEN= 512\n",
        "TRAIN_BS = 8\n",
        "VALID_BS = 8\n",
        "EPOCHS = 1\n",
        "LR =  1e-05\n",
        "#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class PclData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        print(self.len); print(self.data)\n",
        "        \n",
        "    def __getitem__(self, index):# function compulsory\n",
        "        sent = self.data.text[index]\n",
        "       # sent = \" \".join(sent.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "        sent, \n",
        "        None,\n",
        "        add_special_tokens = True,\n",
        "        max_length=self.max_len,\n",
        "        padding = 'max_length',\n",
        "        return_token_type_ids = True,\n",
        "        truncation=True\n",
        "        )\n",
        "        \n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        \n",
        "        return { 'ids': torch.tensor(ids, dtype=torch.long),\n",
        "               'mask': torch.tensor(mask, dtype=torch.long),\n",
        "                'targets': torch.tensor(self.data.label_[index], dtype=torch.long)\n",
        "                \n",
        "               }\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "        "
      ],
      "metadata": {
        "id": "d3VtkE7WTMP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "train_dataset = df_final.sample(frac= train_size, random_state=20)\n",
        "test_dataset = df_final.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "#train_dataset_1 = [ 0 if (y == 1 or y == 0) else 1 for y in train_dataset ]\n",
        "#test_dataset_1 = [ 0 if (y == 1 or y == 0) else 1 for y in test_dataset ]\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df_final.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = PclData(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = PclData(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BS,\n",
        "               'shuffle':True,\n",
        "               'num_workers': 2}\n",
        "test_params = {'batch_size': VALID_BS,\n",
        "              'shuffle': False,\n",
        "              'num_workers': 2}\n",
        "\n",
        "trainloader = DataLoader(training_set, **train_params)\n",
        "testloader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "zOf9q9aFTY2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaModelClass (torch.nn.Module):\n",
        "    def __init__(self, nclasses):\n",
        "        super(RobertaModelClass, self).__init__()\n",
        "        self.l1 = RobertaModel.from_pretrained('roberta-large')\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, nclasses)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Ry83xgwMn_jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaModelClass(nclasses)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "nxXvGk81pBey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =model.parameters(), lr=LR)\n",
        "def calculate_acc(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct\n"
      ],
      "metadata": {
        "id": "umH-JoN3pVD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs/textclassify_experiment_1')\n"
      ],
      "metadata": {
        "id": "ZX14KNQmrDhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    \n",
        "    model.train()\n",
        "    for _, data in tqdm(enumerate(trainloader, 0)):\n",
        "        ids = data['ids'].to(device, dtype=torch.long)\n",
        "        mask = data['mask'].to(device, dtype=torch.long)\n",
        "        targets = data['targets'].to(device, dtype=torch.long)\n",
        "        \n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calculate_acc(big_idx, targets)\n",
        "        \n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if _ % 50 == 0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            acc_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f'Training Loss per 50 steps: {loss_step}, Training Accuracy: {acc_step}')\n",
        "            writer.add_scalar('training_loss', loss_step, epoch*len(trainloader) +_)\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "    print(f'Total Accuracy Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
        "    print(f'Training Loss Epoch: {epoch_loss}, Training Accuracy Epoch: {epoch_acc}')\n",
        "    \n",
        "    return"
      ],
      "metadata": {
        "id": "6PfZBRYnrHKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "\n",
        "def valid(model, testloader):\n",
        "    model.eval()\n",
        "    n_correct =0; n_wrong =0; total=0; tr_loss =0; nb_tr_steps =0; nb_tr_examples=0\n",
        "    y_pred, y_true = [],[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testloader, 0):\n",
        "            ids = data['ids'].to(device, dtype=torch.long)\n",
        "            mask = data['mask'].to(device, dtype=torch.long)\n",
        "            targets = data['targets'].to(device, dtype=torch.long)\n",
        "            \n",
        "            outputs = model(ids, mask)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calculate_acc(big_idx, targets)\n",
        "\n",
        "            y_true.extend(targets.cpu().detach().numpy())\n",
        "      \n",
        "            y_pred.extend(torch.argmax(outputs, dim=1).cpu().detach().numpy())\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples += targets.size(0)\n",
        "            \n",
        "            if _ % 100 == 0:\n",
        "                loss_step = tr_loss/nb_tr_steps\n",
        "                acc_step = (n_correct*100)/nb_tr_examples\n",
        "                print(f'Validation Loss per 100 steps: {loss_step}, Validation Accuracy per: {acc_step}')   \n",
        "        \n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_acc = (n_correct*100)/nb_tr_examples\n",
        "    print(f'Validation Loss Epoch: {epoch_loss}, Validation Accuracy Epoch: {epoch_acc}')\n",
        "    \n",
        "    return epoch_acc, y_true, y_pred"
      ],
      "metadata": {
        "id": "ONoUGedzrOCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "5280kYfwrThg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, y_true, y_pred = valid(model, testloader)"
      ],
      "metadata": {
        "id": "XRlfHpozrXDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nuypq3rGub0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}